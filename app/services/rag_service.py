import json
import asyncio
from typing import AsyncGenerator, List
from pydantic import SecretStr
from langchain_openai import ChatOpenAI
from pymilvus import connections
from langchain_core.embeddings import Embeddings 
from app.core.model_loader import get_embedding_model 

from langchain_milvus import Milvus
from langchain_core.vectorstores import VectorStoreRetriever
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler
from langchain_core.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from sqlalchemy.orm import Session
from app.core.config import settings
from app.models.user import User
from app.models.chat import Conversation, Message
from app.services.cache_service import get_cache, set_cache


# 1. å®šä¹‰ä¸€ä¸ªé€‚é…å™¨ç±»
# ä½œç”¨ï¼šæŠŠæˆ‘ä»¬é¢„åŠ è½½çš„ SentenceTransformer "è£¸æ¨¡å‹" 
# åŒ…è£…æˆ LangChain è®¤è¯†çš„ "Embeddings" å¯¹è±¡
class GlobalLazyEmbeddings(Embeddings):
    def __init__(self):
        model = get_embedding_model()
        
        if model is None:
            raise ValueError("Fatal Error: Embedding model failed to initialize. Please check docker logs.")
            
        self.model = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # è°ƒç”¨ sentence-transformers çš„ encode æ–¹æ³•
        # normalize_embeddings=True å»ºè®®å¼€å¯ï¼Œå¯¹ä½™å¼¦ç›¸ä¼¼åº¦æ›´å¥½
        embeddings = self.model.encode(texts, normalize_embeddings=True)
        return embeddings.tolist()

    def embed_query(self, text: str) -> List[float]:
        # å•æ¡æŸ¥è¯¢ç¼–ç 
        embedding = self.model.encode(text, normalize_embeddings=True)
        return embedding.tolist()
    

def get_retriever(collection_name: str = settings.COLLECTION_NAME, k: int = 5) -> VectorStoreRetriever:
    """è¿æ¥ Milvus å¹¶è¿”å›æ£€ç´¢å™¨"""
    print(f"æ­£åœ¨å°è¯•å»ºç«‹åº•å±‚è¿æ¥ -> {settings.MILVUS_HOST}:{settings.MILVUS_PORT}")
    
    # å¼ºåˆ¶ PyMilvus å»ºç«‹åä¸º "default" çš„è¿æ¥ï¼Œè¿™æ · LangChain å†…éƒ¨ä¼šç›´æ¥å¤ç”¨å®ƒ
    try:
        if not connections.has_connection("default"):
            connections.connect(
                alias="default", 
                host=settings.MILVUS_HOST, 
                port=settings.MILVUS_PORT
            )
            print(" åº•å±‚ PyMilvus è¿æ¥æˆåŠŸï¼")
    except Exception as e:
        print(f" åº•å±‚è¿æ¥å¤±è´¥: {e}")

    embeddings = GlobalLazyEmbeddings()
    
    from pymilvus import Collection
    try:
        col = Collection(collection_name)
        col.load()
        print(f"é›†åˆ {collection_name} å·²åŠ è½½è‡³å†…å­˜")
    except Exception as e:
        print(f"åŠ è½½é›†åˆå¤±è´¥: {e}")
        
    # åœ¨åˆå§‹åŒ– Milvus æ—¶ï¼Œæ˜¾å¼æŒ‡å®š connection_args
    vector_store = Milvus(
        embedding_function=embeddings,
        collection_name=collection_name,
        connection_args={
            "host": settings.MILVUS_HOST,
            "port": str(settings.MILVUS_PORT), # ç¡®ä¿ç«¯å£æ˜¯å­—ç¬¦ä¸²
            "alias": "default" # å¼ºåˆ¶å…³è”æˆ‘ä»¬ä¸Šé¢å»ºç«‹çš„è¿æ¥
        },
        auto_id=True
    )
    return vector_store.as_retriever(search_kwargs={"k": k})

async def _save_chat_to_db(db: Session, user_id: int, question: str, answer: str, sources: List = None):
    """
    (å†…éƒ¨è¾…åŠ©å‡½æ•°) å°†é—®ç­”è®°å½•ä¿å­˜åˆ° MySQL
    """
    try:
        # 1. åˆ›å»ºæ–°ä¼šè¯
        new_conversation = Conversation(
            user_id=user_id,
            title=question[:30] # æˆªå–å‰30å­—ä½œä¸ºæ ‡é¢˜
        )
        db.add(new_conversation)
        db.commit()
        db.refresh(new_conversation)

        # 2. ä¿å­˜ç”¨æˆ·æé—®
        user_msg = Message(
            conversation_id=new_conversation.id,
            role="user",
            content=question
        )
        db.add(user_msg)

        # 3. ä¿å­˜ AI å›ç­” (ğŸŒŸ æŠŠ sources å­˜è¿›å»)
        ai_msg = Message(
            conversation_id=new_conversation.id,
            role="assistant",
            content=answer,
            sources=sources # <--- å…³é”®ç‚¹ï¼šè¿™é‡Œå¿…é¡»æŠŠ sources ä¼ ç»™æ•°æ®åº“æ¨¡å‹
        )
        db.add(ai_msg)
        
        db.commit()
        print(f"--- DEBUG: ä¼šè¯ä¿å­˜æˆåŠŸ (ID: {new_conversation.id}) ---")
    except Exception as e:
        print(f" ä¿å­˜æ•°æ®åº“å¤±è´¥: {e}")
        db.rollback()

async def stream_rag_answer(
    question: str,
    llm_api_key: SecretStr,
    llm_base_url: str,
    llm_model: str,
    db: Session,       
    user: User,
    collection_name: str = settings.COLLECTION_NAME
) -> AsyncGenerator[str, None]:
    """
    RAG æ ¸å¿ƒé€»è¾‘ï¼šç¼“å­˜ -> æ£€ç´¢ Milvus -> æ„é€  Prompt -> æµå¼è°ƒç”¨ LLM -> æŒä¹…åŒ–
    """
    print(f"--- DEBUG: è¿›å…¥ RAG æµç¨‹ï¼Œé—®é¢˜: {question} ---")
    
    # === 1. æ£€æŸ¥ Redis ç¼“å­˜ ===
    cached_data = await get_cache(question)
    
    if cached_data:
        # --- å‘½ä¸­ç¼“å­˜åˆ†æ”¯ ---
        answer = cached_data["answer"]
        yield answer
        
        # è¾“å‡ºå¼•ç”¨æ¥æº
        yield "\n\n---SOURCES---\n"
        for src in cached_data["sources"]:
             yield json.dumps(src, ensure_ascii=False) + "\n"
        
        # å…³é”®ä¿®å¤ï¼šå³ä½¿å‘½ä¸­ç¼“å­˜ï¼Œä¹Ÿè¦ä¿å­˜åˆ° MySQL å†å²è®°å½•
        await _save_chat_to_db(db, user.id, question, answer, sources=cached_data.get("sources"))
        return

    # === 2. ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹ RAG ===
    
    # è·å–æ£€ç´¢å™¨
    try:
        retriever = get_retriever(collection_name=collection_name)
        # ä¿®å¤ï¼šä¹‹å‰è¿™é‡Œè°ƒç”¨äº†ä¸¤æ¬¡ invokeï¼Œåˆ æ‰äº†ä¸€æ¬¡
        print("--- DEBUG: å¼€å§‹æ£€ç´¢... ---")
        docs = retriever.invoke(question)
        print(f"--- DEBUG: æ£€ç´¢åˆ° {len(docs)} ç¯‡æ–‡æ¡£ ---")
        if not docs:
            print("--- DEBUG: æ²¡æœåˆ°åŒ¹é…ï¼Œæ­£åœ¨è·å–æ¨èå‚è€ƒ... ---")
            # æ–¹æ¡ˆï¼šå°è¯•æ£€ç´¢æœ€æ³›åŒ–çš„å†…å®¹ï¼Œæˆ–è€…ç›´æ¥ä»æ–‡ä»¶åˆ—è¡¨é‡Œæ‹¿å‰3ä¸ª
            try:
                # å†æ¬¡æ£€ç´¢ï¼Œä½†è¿™æ¬¡ç”¨ä¸€ä¸ªæç®€çš„å…³é”®è¯æˆ–è€…ç©ºå­—ç¬¦ä¸²ï¼Œè·å–ä¸€äº›åŸºç¡€èµ„æ–™
                recommend_docs = retriever.invoke("åŸºç¡€çŸ¥è¯†") 
                docs = recommend_docs[:3] # å–å‰3ä¸ªä½œä¸ºå¯èƒ½ç›¸å…³çš„èµ„æ–™
            except:
                docs = []
                
    except Exception as e:
        print(f"æ£€ç´¢å‡ºé”™: {e}")
        docs = []

    # if not docs:
    #     yield "åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œæ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
    #     return

    # åˆå§‹åŒ– LLM
    callback = AsyncIteratorCallbackHandler()
    llm = ChatOpenAI(
        api_key=llm_api_key,
        base_url=llm_base_url,
        model=llm_model,
        temperature=0.3,
        streaming=True,
        callbacks=[callback]
    ) # type: ignore

    # æ„é€  Chain
    template = """
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç»“åˆ[å‚è€ƒèµ„æ–™]å›ç­”ç”¨æˆ·çš„[é—®é¢˜]ã€‚
    
    [è§„åˆ™]:
    1. å¦‚æœ[å‚è€ƒèµ„æ–™]ä¸é—®é¢˜é«˜åº¦ç›¸å…³ï¼Œè¯·è¯¦ç»†å›ç­”å¹¶å¼•ç”¨ã€‚
    2. å¦‚æœ[å‚è€ƒèµ„æ–™]ä¸é—®é¢˜ç›¸å…³åº¦è¾ƒä½ï¼Œè¯·å…ˆå›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶è¯´æ˜ï¼šâ€œä»¥ä¸‹æ˜¯å¯èƒ½ä¸æ‚¨é—®é¢˜ç›¸å…³çš„å‚è€ƒèµ„æ–™ï¼Œä¾›æ‚¨å‚è€ƒã€‚â€
    3. å¦‚æœæ²¡æœ‰å®Œå…¨åŒ¹é…çš„å†…å®¹ï¼Œè¯·æ ¹æ®ä½ çš„é€šç”¨çŸ¥è¯†å›ç­”ï¼Œå¹¶åœ¨ç»“å°¾åˆ—å‡ºå¯èƒ½ç›¸å…³çš„æ–‡ä»¶ã€‚
    
    [å‚è€ƒèµ„æ–™]:
    {context}

    [é—®é¢˜]:
    {input}

    å›ç­”:
    """
    prompt = PromptTemplate.from_template(template)
    qa_chain = create_stuff_documents_chain(llm, prompt)
    
    # å¯åŠ¨ç”Ÿæˆä»»åŠ¡
    task = asyncio.create_task(
        qa_chain.ainvoke({"input": question, "context": docs})
    )
    
    full_answer = ""
    doc_metadatas = [doc.metadata for doc in docs]

    try:
        async for token in callback.aiter():
            full_answer += token
            yield token
    except Exception as e:
        print(f"LLM ç”Ÿæˆå‡ºé”™: {e}")
        yield f"\n[ç”Ÿæˆä¸­æ–­: {e}]"
    finally:
        await task 
        
        # è¾“å‡ºå¼•ç”¨æ¥æº
        yield "\n\n---SOURCES---\n"
        for meta in doc_metadatas:
            yield json.dumps(meta, ensure_ascii=False) + "\n"

        # === 3. æ”¶å°¾å·¥ä½œï¼šå†™å…¥ç¼“å­˜ & å†™å…¥æ•°æ®åº“ ===
        if full_answer:
            # å†™å…¥ Redis (å¼‚æ­¥åå°æ‰§è¡Œï¼Œä¸é˜»å¡)
            asyncio.create_task(set_cache(question, full_answer, doc_metadatas))
            
            # å†™å…¥ MySQL (ç¡®ä¿è®°å½•å†å²)
            await _save_chat_to_db(db, user.id, question, full_answer, sources=doc_metadatas)

    print("--- DEBUG: RAG æµç¨‹ç»“æŸ ---")