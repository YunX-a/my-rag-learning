# app/services/rag_service.py
import json
import asyncio
from typing import AsyncGenerator, List, Optional, Any
from pydantic import SecretStr
from langchain_openai import ChatOpenAI
from pymilvus import connections
from langchain_core.embeddings import Embeddings 
from app.core.model_loader import get_embedding_model 

from langchain_milvus import Milvus
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.documents import Document

from sqlalchemy.orm import Session
from app.core.config import settings
from app.models.user import User
from app.models.chat import Conversation, Message
from app.services.cache_service import get_cache, set_cache
# å¼•å…¥ ES æœåŠ¡ (ç¡®ä¿ä½ å·²ç»åˆ›å»ºäº† app/services/es_service.py)
from app.services.es_service import search_keyword

# --- 1. å®šä¹‰é€‚é…å™¨ç±» ---
class GlobalLazyEmbeddings(Embeddings):
    def __init__(self):
        model = get_embedding_model()
        if model is None:
            raise ValueError("Fatal Error: Embedding model failed to initialize.")
        self.model = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        embeddings = self.model.encode(texts, normalize_embeddings=True)
        return embeddings.tolist()

    def embed_query(self, text: str) -> List[float]:
        embedding = self.model.encode(text, normalize_embeddings=True)
        return embedding.tolist()

# --- 2. RRF èåˆç®—æ³• (æ ¸å¿ƒæ–°å¢) ---
def reciprocal_rank_fusion(results: List[List[Any]], k=60):
    """
    RRF èåˆç®—æ³•ï¼šåˆå¹¶å¤šè·¯æ£€ç´¢ç»“æœ
    :param results: å¤šä¸ªåˆ—è¡¨ï¼ŒåŒ…å« Document æˆ– ES hit å¯¹è±¡
    """
    fused_scores = {}
    
    for doc_list in results:
        for rank, item in enumerate(doc_list):
            # ç»Ÿä¸€è½¬æ¢ä¸ºå†…å®¹å­—ç¬¦ä¸²å’Œå…ƒæ•°æ®å­—ç¬¦ä¸²ä½œä¸ºå”¯ä¸€ Key
            if isinstance(item, Document):
                content = item.page_content
                # json dumps ä¿è¯å­—å…¸é¡ºåºä¸€è‡´ï¼Œä½œä¸ºå”¯ä¸€æ ‡è¯†çš„ä¸€éƒ¨åˆ†
                meta_str = json.dumps(item.metadata, sort_keys=True, ensure_ascii=False)
            else:
                # å…¼å®¹å¯èƒ½çš„å…¶ä»–æ ¼å¼
                content = str(item)
                meta_str = "{}"

            key = (content, meta_str)
            
            if key not in fused_scores:
                fused_scores[key] = 0
            
            # RRF å…¬å¼: score = 1 / (rank + k)
            fused_scores[key] += 1 / (rank + k)
            
    # æŒ‰åˆ†æ•°å€’åºæ’åˆ—
    reranked = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    
    # è¿˜åŸä¸º Document å¯¹è±¡
    final_docs = []
    for (content, meta_str), score in reranked:
        try:
            meta = json.loads(meta_str)
        except:
            meta = {}
        # å¯ä»¥åœ¨ metadata é‡ŒæŠŠ score åŠ ä¸Šï¼Œæ–¹ä¾¿è°ƒè¯•
        meta["rrf_score"] = score
        final_docs.append(Document(page_content=content, metadata=meta))
    
    return final_docs

# --- 3. æ•°æ®åº“è¾…åŠ©å‡½æ•° ---
async def _save_chat_to_db(
    db: Session, 
    user_id: int, 
    question: str, 
    answer: str, 
    sources: Optional[List[Any]] = None
):
    try:
        new_conversation = Conversation(user_id=user_id, title=question[:30])
        db.add(new_conversation)
        db.commit()
        db.refresh(new_conversation)

        user_msg = Message(
            conversation_id=new_conversation.id,
            role="user",
            content=question
        )
        db.add(user_msg)

        ai_msg = Message(
            conversation_id=new_conversation.id,
            role="assistant",
            content=answer,
            sources=sources 
        )
        db.add(ai_msg)
        
        db.commit()
    except Exception as e:
        print(f"ä¿å­˜æ•°æ®åº“å¤±è´¥: {e}")
        db.rollback()

def get_retriever(collection_name: str = settings.COLLECTION_NAME, k: int = 5) -> VectorStoreRetriever:
    try:
        if not connections.has_connection("default"):
            connections.connect(
                alias="default", 
                host=settings.MILVUS_HOST, 
                port=settings.MILVUS_PORT
            )
    except Exception as e:
        print(f"åº•å±‚è¿æ¥è­¦å‘Š: {e}")

    embeddings = GlobalLazyEmbeddings()
    
    vector_store = Milvus(
        embedding_function=embeddings,
        collection_name=collection_name,
        connection_args={
            "host": settings.MILVUS_HOST,
            "port": str(settings.MILVUS_PORT), 
            "alias": "default" 
        },
        auto_id=True
    )
    return vector_store.as_retriever(search_kwargs={"k": k})

# --- 4. æ ¸å¿ƒ RAG é€»è¾‘ (æ··åˆæ£€ç´¢ç‰ˆ) ---
async def stream_rag_answer(
    question: str,
    llm_api_key: SecretStr,
    llm_base_url: str,
    llm_model: str,
    db: Session,       
    user: User,
    collection_name: str = settings.COLLECTION_NAME
) -> AsyncGenerator[str, None]:
    
    print(f"--- Hybrid RAG Start: {question} ---")
    
    # === 1. æ£€æŸ¥ Redis ç¼“å­˜ ===
    cached_data = await get_cache(question)
    if cached_data:
        yield cached_data["answer"]
        yield "\n\n---SOURCES---\n"
        cached_sources = cached_data.get("sources")
        if cached_sources:
            for sz in cached_sources:
                 yield json.dumps(sz, ensure_ascii=False) + "\n"
        await _save_chat_to_db(db, user.id, question, cached_data["answer"], sources=cached_sources)
        return

    # === 2. æ··åˆæ£€ç´¢ (Hybrid Search) ===
    try:
        # A. å‘é‡æ£€ç´¢ (Milvus)
        print("ğŸ” æ‰§è¡Œ Milvus å‘é‡æ£€ç´¢...")
        retriever = get_retriever(collection_name=collection_name, k=5)
        milvus_docs = retriever.invoke(question)
        
        # B. å…³é”®è¯æ£€ç´¢ (ES)
        print("ğŸ” æ‰§è¡Œ ES å…³é”®è¯æ£€ç´¢...")
        es_hits = search_keyword(question, k=5)
        es_docs = []
        for hit in es_hits:
            source = hit["_source"]
            # ç»Ÿä¸€è½¬ä¸º Document
            es_docs.append(Document(
                page_content=source.get("content", ""),
                metadata={k: v for k, v in source.items() if k != "content"}
            ))

        # C. RRF èåˆ
        print(f"âš—ï¸ æ‰§è¡Œ RRF èåˆ (å‘é‡: {len(milvus_docs)}, å…³é”®è¯: {len(es_docs)})...")
        final_docs = reciprocal_rank_fusion([milvus_docs, es_docs])
        
        # å–å‰ 6 ä¸ª
        used_docs = final_docs[:6]
        
        if used_docs:
            context_text = "\n\n------\n\n".join([d.page_content for d in used_docs])
        else:
            context_text = "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·ä¾æ®ä½ çš„é€šç”¨çŸ¥è¯†å›ç­”ã€‚"

    except Exception as e:
        print(f"æ£€ç´¢è¿‡ç¨‹å‡ºé”™: {e}")
        used_docs = []
        context_text = ""
    
    # [å…³é”®ä¿®å¤]ï¼šåœ¨è¿™é‡Œæ ¹æ® used_docs å®šä¹‰ doc_metadatasï¼Œä¾›åç»­ä½¿ç”¨
    doc_metadatas = [doc.metadata for doc in used_docs]

    # === 3. æ„å»º Prompt ===
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åº“åŠ©æ‰‹ã€‚
è¯·ç»“åˆä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™ã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
å¦‚æœå‚è€ƒèµ„æ–™ä¸­æœ‰å¤šä¸ªè§‚ç‚¹ï¼Œè¯·ç»¼åˆå›ç­”ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘:
{context_text}

è¦æ±‚ï¼š
1. å¼•ç”¨èµ„æ–™ä¸­çš„äº‹å®æ¥æ”¯æŒä½ çš„è§‚ç‚¹ã€‚
2. å¦‚æœèµ„æ–™ä¸è¶³ï¼Œè¯·è¯šå®è¯´æ˜ã€‚
"""

    # === 4. ç”Ÿæˆ (Generation) ===
    llm = ChatOpenAI(
        api_key=llm_api_key,
        base_url=llm_base_url,
        model=llm_model,
        temperature=0.3,
        streaming=True
    )

    full_answer = ""
    
    try:
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=question)
        ]
        
        async for chunk in llm.astream(messages):
            content = chunk.content
            if content:
                full_answer += str(content)
                yield str(content)
                
    except Exception as e:
        print(f"LLM ç”Ÿæˆå‡ºé”™: {e}")
        yield f"\n[ç”Ÿæˆä¸­æ–­: {e}]"
    
    # === 5. æ”¶å°¾ ===
    yield "\n\n---SOURCES---\n"
    # è¿™é‡Œ doc_metadatas å·²ç»è¢«æ­£ç¡®å®šä¹‰äº†
    for meta in doc_metadatas:
        yield json.dumps(meta, ensure_ascii=False) + "\n"

    if full_answer:
        asyncio.create_task(set_cache(question, full_answer, doc_metadatas))
        await _save_chat_to_db(db, user.id, question, full_answer, sources=doc_metadatas)

    print("--- RAG End ---")